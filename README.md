# Adversarial-DL
## Papers
* [MetaAdvDet: Towards Robust Detection of Evolving Adversarial Attacks](https://arxiv.org/abs/1908.02199)
* [Connecting Lyapunov Control Theory to Adversarial Attacks](https://arxiv.org/abs/1907.07732)
* [Enhancing Adversarial Example Transferability with an Intermediate Level Attack](https://arxiv.org/abs/1907.10823)
* [advPattern: Physical-World Attacks on Deep Person Re-Identification via Adversarially Transformable Patterns](https://arxiv.org/abs/1908.09327)
* [On the Robustness of Semantic Segmentation Models to Adversarial Attacks](http://openaccess.thecvf.com/content_cvpr_2018/html/Arnab_On_the_Robustness_CVPR_2018_paper.html)
* [The Attack Generator: A Systematic Approach Towards Constructing Adversarial Attacks](https://arxiv.org/abs/1906.07077)
* [Copy and Paste: A Simple But Effective Initialization Method for Black-Box Adversarial Attacks](https://arxiv.org/abs/1906.06086)
* [AdvHat: Real-world adversarial attack on ArcFace Face ID system](https://arxiv.org/abs/1908.08705)
* [Adversarial Attacks on Neural Networks for Graph Data](https://arxiv.org/abs/1805.07984)
* [Adversarial Attack on Graph Structured Data](https://arxiv.org/abs/1806.02371)
## CCS
* [Attacking Graph-based Classification via Manipulating the Graph Structure](https://arxiv.org/abs/1903.00553)
* [Adversarial Sensor Attack on LiDAR-based Perception in Autonomous Driving](https://arxiv.org/abs/1907.06826)
## Arxiv
* [Adversarial learning](https://dl.acm.org/citation.cfm?id=1081950)
* [On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)
* [Customizing an Adversarial Example Generator with Class-Conditional GANs](https://arxiv.org/abs/1806.10496)
* [Generating Adversarial Examples with Adversarial Networks](https://arxiv.org/abs/1801.02610)
* [Defending Against Adversarial Attacks by Leveraging an Entire GAN](https://arxiv.org/abs/1805.10652)
* [Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models](https://arxiv.org/abs/1712.04248)
* [Evasion Attacks against Machine Learning at Test Time](https://arxiv.org/abs/1708.06131)
* [Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness](https://arxiv.org/abs/1903.10484)
* [SentiNet: Detecting Physical Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292)
* [Sitatapatra: Blocking the Transfer of Adversarial Samples](https://arxiv.org/abs/1901.08121)
* [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)
* [Wasserstein Adversarial Examples via Projected Sinkhorn Iterations](https://arxiv.org/abs/1902.07906)
* [Real-Time Adversarial Attacks](https://arxiv.org/abs/1905.13399)
* [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models](https://arxiv.org/abs/1805.06605)
* [Robust Graph Neural Network Against Poisoning Attacks via Transfer Learning](https://arxiv.org/abs/1908.07558)
* [AdvFaces: Adversarial Face Synthesis](https://arxiv.org/abs/1908.05008)
* [Adversarial Examples on Graph Data: Deep Insights into Attack and Defense](https://arxiv.org/abs/1903.01610)
* [Data Poisoning against Differentially-Private Learners: Attacks and Defenses](https://arxiv.org/abs/1903.09860)
* [Data Poisoning Attack against Knowledge Graph Embedding](https://arxiv.org/abs/1904.12052)
* [Robust Audio Adversarial Example for a Physical Attack](https://arxiv.org/abs/1810.11793)
* [Adversarial Defense Framework for Graph Neural Network](https://arxiv.org/abs/1905.03679)
* [The General Black-box Attack Method for Graph Neural Networks](https://arxiv.org/abs/1908.01297)
* [Adversarial Attack and Defense on Graph Data: A Survey](https://arxiv.org/abs/1812.10528)
* [Open DNN Box by Power Side-Channel Attack](https://arxiv.org/abs/1907.10406)
* [Transferable Adversarial Attacks for Image and Video Object Detection](https://arxiv.org/abs/1811.12641)
* [Exploring Connections Between Active Learning and Model Extraction](https://arxiv.org/abs/1811.02054)
* [A framework for the extraction of Deep Neural Networks by leveraging public data](https://arxiv.org/abs/1905.09165)
* [Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey](https://arxiv.org/abs/1801.00553)
* [Explaining and harnessing adversarial examples]
* [Adversarial Edit Attacks for Tree Data]
* [Model Extraction and Active Learning]
* [High-Fidelity Extraction of Neural Network Models]
* [LASSIFIERS A GAINST A DVERSARIAL EXAMPLES]

## USENIX
* [Hybrid Batch Attacks: Finding Black-box Adversarial Examples with Limited Queries](https://arxiv.org/abs/1908.07000)
* [TreeHuggr: Discovering Where Tree-based Classifiers are Vulnerable to Adversarial Attack](https://www.usenix.org/conference/scainet19/presentation/filar)
* [Local Model Poisoning Attacks to Byzantine-Robust Federated Learning](https://arxiv.org/abs/1911.11815)
