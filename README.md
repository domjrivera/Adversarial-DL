# Adversarial-DL

## Arxiv

* [On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)

* [Customizing an Adversarial Example Generator with Class-Conditional GANs](https://arxiv.org/abs/1806.10496)

* [Generating Adversarial Examples with Adversarial Networks](https://arxiv.org/abs/1801.02610)

* [Defending Against Adversarial Attacks by Leveraging an Entire GAN](https://arxiv.org/abs/1805.10652)

* [Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models](https://arxiv.org/abs/1712.04248)

* [Evasion Attacks against Machine Learning at Test Time](https://arxiv.org/abs/1708.06131)

* [Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness](https://arxiv.org/abs/1903.10484)

* [SentiNet: Detecting Physical Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292)

* [Sitatapatra: Blocking the Transfer of Adversarial Samples](https://arxiv.org/abs/1901.08121)

* [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)

* [Wasserstein Adversarial Examples via Projected Sinkhorn Iterations](https://arxiv.org/abs/1902.07906)

* [Real-Time Adversarial Attacks](https://arxiv.org/abs/1905.13399)

* [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models](https://arxiv.org/abs/1805.06605)

* [Robust Graph Neural Network Against Poisoning Attacks via Transfer Learning](https://arxiv.org/abs/1908.07558)

* [AdvFaces: Adversarial Face Synthesis](https://arxiv.org/abs/1908.05008)

* [Adversarial Examples on Graph Data: Deep Insights into Attack and Defense](https://arxiv.org/abs/1903.01610)

* [Data Poisoning against Differentially-Private Learners: Attacks and Defenses](https://arxiv.org/abs/1903.09860)

* [Data Poisoning Attack against Knowledge Graph Embedding](https://arxiv.org/abs/1904.12052)

* [Robust Audio Adversarial Example for a Physical Attack](https://arxiv.org/abs/1810.11793)

* [Adversarial Defense Framework for Graph Neural Network](https://arxiv.org/abs/1905.03679)

* [The General Black-box Attack Method for Graph Neural Networks](https://arxiv.org/abs/1908.01297)

* [Adversarial Attack and Defense on Graph Data: A Survey](https://arxiv.org/abs/1812.10528)

* [Open DNN Box by Power Side-Channel Attack](https://arxiv.org/abs/1907.10406)

* [Transferable Adversarial Attacks for Image and Video Object Detection](https://arxiv.org/abs/1811.12641)

## USENIX

* [TreeHuggr: Discovering Where Tree-based Classifiers are Vulnerable to Adversarial Attack](https://www.usenix.org/conference/scainet19/presentation/filar)