# Adversarial-DL


* [On Evaluating Adversarial Robustness](https://arxiv.org/abs/1902.06705)

* [Customizing an Adversarial Example Generator with Class-Conditional GANs](https://arxiv.org/abs/1806.10496)

* [Generating Adversarial Examples with Adversarial Networks](https://arxiv.org/abs/1801.02610)

* [Defending Against Adversarial Attacks by Leveraging an Entire GAN](https://arxiv.org/abs/1805.10652)

* [Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models](https://arxiv.org/abs/1712.04248)

* [Evasion Attacks against Machine Learning at Test Time](https://arxiv.org/abs/1708.06131)

* [Exploiting Excessive Invariance caused by Norm-Bounded Adversarial Robustness](https://arxiv.org/abs/1903.10484)

* [SentiNet: Detecting Physical Attacks Against Deep Learning Systems](https://arxiv.org/abs/1812.00292)

* [Sitatapatra: Blocking the Transfer of Adversarial Samples](https://arxiv.org/abs/1901.08121)

* [Adversarial Examples Are Not Bugs, They Are Features](https://arxiv.org/abs/1905.02175)

* [Wasserstein Adversarial Examples via Projected Sinkhorn Iterations](https://arxiv.org/abs/1902.07906)

* [Real-Time Adversarial Attacks](https://arxiv.org/abs/1905.13399)

* [Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models](https://arxiv.org/abs/1805.06605)

* [Robust Graph Neural Network Against Poisoning Attacks via Transfer Learning](https://arxiv.org/abs/1908.07558)

* [AdvFaces: Adversarial Face Synthesis](https://arxiv.org/abs/1908.05008)* [